{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5848b3b",
   "metadata": {},
   "source": [
    "## 3.2 Character Identification (01_character_identification.ipynb)\n",
    "\n",
    "**Objective**: The primary goal of this stage was to process the cleaned plain text from the novels, identify all mentions of characters, filter out noise and non-character entities, and consolidate different textual references (e.g., \"Jonathan Harker\", \"Jonathan\") into unique character representations.\n",
    "\n",
    "**Design & Implementation Strategy**: Since the pipeline starts from plain text without leveraging pre-existing mention annotations (like those in the Muzny et al. XML), a Named Entity Recognition (NER) model was employed as the core component for identifying potential character mentions. This was followed by filtering and consolidation steps.\n",
    "\n",
    "### Key Stages & Implementation Details:\n",
    "\n",
    "1. **Named Entity Recognition (NER) using BERT**:\n",
    "\n",
    "   - **Model**: A transformer-based NER model specifically fine-tuned on literary texts, compnet-renard/bert-base-cased-literary-NER, was utilized via the Hugging Face transformers library's pipeline function. This model was chosen for its reported effectiveness on literary domain text compared to general-purpose NER models.\n",
    "\n",
    "   - **Chunking**: Due to the limitations of BERT-based models on long input sequences, the cleaned text of each novel was processed in overlapping chunks.\n",
    "\n",
    "     - chunk_size: 2000 characters (heuristic, may require tuning per model/hardware).\n",
    "     \n",
    "     - overlap: 200 characters (to ensure entities spanning chunk boundaries are captured).\n",
    "\n",
    "   - **Aggregation**: The aggregation_strategy=\"first\" was used within the pipeline. This strategy attempts to merge sub-word tokens back into complete entities and, in cases of ambiguity or overlapping predictions for the same entity type, tends to favour the prediction associated with the first sub-word token.\n",
    "\n",
    "   - **Output**: All identified entities tagged as 'PER' (Person) were extracted along with their text and character offsets relative to the original full text. Simple deduplication based on exact text and offsets was performed to handle entities detected identically in overlapping chunk regions. The raw, unique PERSON mentions were initially saved to ../data/ner_person_mentions_bert.json for traceability.\n",
    "\n",
    "2. **Post-NER Filtering**:\n",
    "\n",
    "   - **Rationale**: Raw NER output often contains noise or misclassifications (e.g., standalone titles, punctuation). Filtering is necessary to improve the quality of mentions passed to the consolidation stage.\n",
    "\n",
    "   - **Rules Applied**:\n",
    "\n",
    "     - Mentions consisting solely of punctuation or whitespace after stripping were removed.\n",
    "\n",
    "     - Mentions consisting only of a common standalone title (e.g., \"Mr\", \"Mrs\", \"Lady\", case-insensitive, after stripping punctuation) were removed.\n",
    "\n",
    "   - **Output**: The cleaned and filtered list of PERSON mentions was saved to ../data/ner_person_mentions_bert_filtered.json.\n",
    "\n",
    "3. **Character Consolidation**:\n",
    "\n",
    "   - **Goal**: Group the filtered mentions that likely refer to the same character entity.\n",
    "\n",
    "   - **Primary Method (nicknames library)**: The Python nicknames library was used. For each mention's text (lowercased), it attempted to find a canonical/formal name (e.g., mapping \"lizzy\" to \"elizabeth\"). If found, the canonical name provided by the library was used as the initial grouping key.\n",
    "\n",
    "   - **Fallback Method (Rule-based)**: If a mention was not found in the nicknames database:\n",
    "\n",
    "     - Common titles (Mr, Mrs, etc.) were stripped from the beginning of the mention text.\n",
    "\n",
    "     - The remaining text (or the original cleaned text if no title was stripped) was lowercased and used as the grouping key.\n",
    "\n",
    "   - **Variation Collection**: The original filtered mention text (before lowercasing for nickname/fallback lookup) was added to a set associated with the determined grouping key. This preserves the different ways a character was referred to.\n",
    "\n",
    "   - **Final Canonical Key Generation**: After grouping, a final representative canonical_key was generated for each group. This was done by identifying the most frequent variation within the group's collected mentions. This most frequent form was then formatted (e.g., stripping/prefixing titles, Title_Casing remaining parts, joining with underscores like Mr_Jonathan_Harker or Mina).\n",
    "\n",
    "   - **Output**: A consolidated list mapping final canonical keys to their total mention count and the set of variations observed was saved to ../data/character_analysis_consolidated_nicknames.csv.\n",
    "\n",
    "### Libraries & Tools Used:\n",
    "\n",
    "- **transformers** (Hugging Face): For loading and running the NER model.\n",
    "\n",
    "- **pandas**: For data manipulation and saving CSV outputs.\n",
    "\n",
    "- **nicknames**: For dictionary-based nickname-to-canonical name mapping.\n",
    "\n",
    "- **Standard Python libraries**: json, re, collections, os, string.\n",
    "\n",
    "### Self-Correction/Improvements from previous versions mentioned in discussion:\n",
    "\n",
    "- This version correctly describes using the BERT NER model and the nicknames library, not the spaCy NER and direct XML alias mapping from earlier attempts.\n",
    "\n",
    "- It details the chunking and filtering steps which are present in your current code.\n",
    "\n",
    "- It clarifies the consolidation process (nicknames first, then rule-based fallback)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ef47ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import libraries\n",
    "import spacy\n",
    "import os \n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET # Added for XML parsing (used in the first attempts when prinde and prejudice was used for alias mapping)\n",
    "\n",
    "print(\"Libraries imported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dc751b",
   "metadata": {},
   "source": [
    "## Load Cleaned Text\n",
    "Read the content of the cleaned text file created by `00_pre_proc.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44ef9f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded cleaned text from: ../data/dracula_cleaned.txt\n",
      "Text length: 848415 characters\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load cleaned text\n",
    "input_file_path = \"../data/dracula_cleaned.txt\" # to edit\n",
    "\n",
    "try:\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "        cleaned_text = file.read()\n",
    "    print(f\"Successfully loaded cleaned text from: {input_file_path}\")\n",
    "    print(f\"Text length: {len(cleaned_text)} characters\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Cleaned text file not found at {input_file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred loading the file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "process_text_md",
   "metadata": {},
   "source": [
    "## Process Text for Named Entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extract_persons_md",
   "metadata": {},
   "source": [
    "## Extract PERSON Entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84777f81",
   "metadata": {},
   "source": [
    "## I should improve NER - third attempt (way better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3dd5db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Starting chunk processing. Total text length: 848415\n",
      "DEBUG: Chunk processing finished.\n",
      "DEBUG: Total PERSON mentions found across all chunks: 2752\n",
      "DEBUG: Unique PERSON mentions after deduplication: 2692\n",
      "Successfully saved 2692 person mentions to ../data/ner_person_mentions_bert.json\n",
      "\n",
      "First few person mentions:\n",
      "{'text': 'Bram Stoker', 'start_char': 19, 'end_char': 30}\n",
      "{'text': 'Bram Stoker', 'start_char': 165, 'end_char': 176}\n",
      "{'text': 'Jonathan Harker', 'start_char': 345, 'end_char': 360}\n",
      "{'text': 'Jonathan Harker', 'start_char': 383, 'end_char': 398}\n",
      "{'text': 'Jonathan Harker', 'start_char': 422, 'end_char': 437}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "# --- Initialize the pipeline ---\n",
    "literary_ner = pipeline(\"ner\", model=\"compnet-renard/bert-base-cased-literary-NER\", aggregation_strategy=\"first\")\n",
    "\n",
    "# --- Define Chunking Parameters  ---.\n",
    "chunk_size = 2000 # Number of characters per chunk (!)\n",
    "overlap = 200    # Number of characters overlap (!)\n",
    "\n",
    "# --- Process in Chunks ---\n",
    "all_person_mentions = []\n",
    "current_pos = 0\n",
    "\n",
    "print(f\"DEBUG: Starting chunk processing. Total text length: {len(cleaned_text)}\")\n",
    "\n",
    "while current_pos < len(cleaned_text):\n",
    "    chunk_start = current_pos\n",
    "    chunk_end = min(current_pos + chunk_size, len(cleaned_text))\n",
    "    text_chunk = cleaned_text[chunk_start:chunk_end]\n",
    "\n",
    "    # print(f\"DEBUG: Processing chunk: {chunk_start} - {chunk_end}\") # Optional debug\n",
    "\n",
    "    if not text_chunk.strip(): # Skip empty chunks if any\n",
    "        current_pos += chunk_size - overlap\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "         # Run NER on the chunk\n",
    "         ner_results_chunk = literary_ner(text_chunk)\n",
    "\n",
    "         # Process results for this chunk\n",
    "         for entity in ner_results_chunk:\n",
    "             # Adjust character offsets to be relative to the full text\n",
    "             original_start = chunk_start + entity['start']\n",
    "             original_end = chunk_start + entity['end']\n",
    "\n",
    "             mention_data = {\n",
    "                 \"text\": entity['word'],\n",
    "                 \"start_char\": original_start,\n",
    "                 \"end_char\": original_end\n",
    "                 \n",
    "             }\n",
    "\n",
    "             if entity.get('entity_group') == 'PER':\n",
    "                 all_person_mentions.append(mention_data)\n",
    "\n",
    "    except Exception as e:\n",
    "         print(f\"ERROR processing chunk {chunk_start}-{chunk_end}: {e}\") # Log errors\n",
    "\n",
    "    # Move to the next chunk position\n",
    "    # If it's the last chunk, stop\n",
    "    if chunk_end == len(cleaned_text):\n",
    "        break\n",
    "    current_pos += chunk_size - overlap # Move forward, maintaining overlap\n",
    "\n",
    "\n",
    "print(f\"DEBUG: Chunk processing finished.\")\n",
    "print(f\"DEBUG: Total PERSON mentions found across all chunks: {len(all_person_mentions)}\")\n",
    "\n",
    "\n",
    "\n",
    "# Simple deduplication based on exact start/end/text match\n",
    "unique_person_mentions_set = set()\n",
    "unique_person_mentions = []\n",
    "for mention in all_person_mentions:\n",
    "    mention_tuple = (mention['text'], mention['start_char'], mention['end_char'])\n",
    "    if mention_tuple not in unique_person_mentions_set:\n",
    "        unique_person_mentions_set.add(mention_tuple)\n",
    "        unique_person_mentions.append(mention)\n",
    "\n",
    "print(f\"DEBUG: Unique PERSON mentions after deduplication: {len(unique_person_mentions)}\")\n",
    "\n",
    "\n",
    "\n",
    "person_mentions = unique_person_mentions # Assign to the variable name used later\n",
    "\n",
    "# After the NER processing and deduplication...\n",
    "if person_mentions:\n",
    "    import json\n",
    "    output_data_path = \"../data/ner_person_mentions_bert.json\"\n",
    "    \n",
    "    # Save to JSON file\n",
    "    try:\n",
    "        with open(output_data_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(person_mentions, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"Successfully saved {len(person_mentions)} person mentions to {output_data_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving to JSON: {e}\")\n",
    "else:\n",
    "    print(\"No person mentions found to save\")\n",
    "\n",
    "# few entries to verify the data\n",
    "print(\"\\nFirst few person mentions:\")\n",
    "for mention in person_mentions[:5]:\n",
    "    print(mention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a68790",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b79ac5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Starting Post-NER Filtering on 2692 mentions...\n",
      "DEBUG: Mentions remaining after filtering: 2338\n",
      "DEBUG: Saving 2338 filtered mentions to ../data/ner_person_mentions_bert_filtered.json...\n",
      "DEBUG: Successfully saved filtered mentions.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# List of standalone titles/honorifics to filter if they appear alone\n",
    "standalone_titles = {\"mr\", \"mrs\", \"miss\", \"ms\", \"dr\", \"lady\", \"sir\", \"colonel\", \"captain\", \"lord\"} # Add more\n",
    "# Characters considered punctuation for stripping/checking\n",
    "import string\n",
    "punctuation_chars = string.punctuation # Gets '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "\n",
    "# --- Implement Post-NER Filtering ---\n",
    "filtered_person_mentions = []\n",
    "if person_mentions: # Use the list generated by the HF model\n",
    "    print(f\"DEBUG: Starting Post-NER Filtering on {len(person_mentions)} mentions...\")\n",
    "    for mention in person_mentions:\n",
    "        original_text = mention['text']\n",
    "\n",
    "        # 1. Basic Cleaning: Remove leading/trailing whitespace and punctuation\n",
    "        #    Example: \". Bennet \" -> \"Bennet\" ; \".\" -> \"\" ; \"Mr.\" -> \"Mr\"\n",
    "        cleaned_text = original_text.strip().strip(punctuation_chars)\n",
    "        cleaned_text_lower = cleaned_text.lower()\n",
    "\n",
    "        # --- Apply Filters ---\n",
    "        # Filter 1: Check if empty after cleaning (e.g., if it was just \".\")\n",
    "        if not cleaned_text:\n",
    "            # print(f\"Filtering empty/punctuation mention: '{original_text}'\") # Optional debug\n",
    "            continue\n",
    "\n",
    "        # Filter 2: Filter if it's just a standalone title\n",
    "        if cleaned_text_lower in standalone_titles:\n",
    "            # print(f\"Filtering standalone title: '{original_text}' -> '{cleaned_text}'\") # Optional debug\n",
    "            continue\n",
    "\n",
    "\n",
    "        \n",
    "        mention_to_add = {\n",
    "            \"text\": cleaned_text,\n",
    "            \"start_char\": mention['start_char'],\n",
    "            \"end_char\": mention['end_char']\n",
    "        }\n",
    "        filtered_person_mentions.append(mention_to_add)\n",
    "\n",
    "    print(f\"DEBUG: Mentions remaining after filtering: {len(filtered_person_mentions)}\")\n",
    "else:\n",
    "     print(\"DEBUG: Initial person_mentions list was empty. Skipping filtering.\")\n",
    "\n",
    "\n",
    "# Saving the FILTERED list ---\n",
    "output_filtered_path = \"../data/ner_person_mentions_bert_filtered.json\"\n",
    "if filtered_person_mentions:\n",
    "    print(f\"DEBUG: Saving {len(filtered_person_mentions)} filtered mentions to {output_filtered_path}...\")\n",
    "    import json\n",
    "    import os\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(output_filtered_path), exist_ok=True)\n",
    "        with open(output_filtered_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(filtered_person_mentions, f, indent=4)\n",
    "        print(f\"DEBUG: Successfully saved filtered mentions.\")\n",
    "    except Exception as e:\n",
    "        print(f\"DEBUG: Error saving filtered JSON: {e}\")\n",
    "else:\n",
    "     print(\"DEBUG: No filtered person mentions to save.\")\n",
    "\n",
    "\n",
    "# next step is consolidation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbac61ef",
   "metadata": {},
   "source": [
    "\n",
    "## Consolidate Characters - by using a nicknames library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b524dadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2338 filtered mentions from ../data/ner_person_mentions_bert_filtered.json\n",
      "NickNamer initialized.\n",
      "Starting character consolidation using NickNamer and rules...\n",
      "Refining canonical keys...\n",
      "\n",
      "Successfully saved consolidated results (nicknames + rules) to '../data/character_analysis_consolidated_nicknames.csv'\n"
     ]
    }
   ],
   "source": [
    "# --- Cell for Character Consolidation  ---\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "from nicknames import NickNamer # Import the library\n",
    "\n",
    "# --- Load Filtered Mentions ---\n",
    "filtered_mentions_path = \"../data/ner_person_mentions_bert_filtered.json\"\n",
    "original_filtered_mentions = []\n",
    "try:\n",
    "    with open(filtered_mentions_path, 'r', encoding='utf-8') as f:\n",
    "        # This loads a LIST of mention dictionaries, e.g., [{'text': 'Bennet', 'start_char': ...}, ...]\n",
    "        original_filtered_mentions = json.load(f)\n",
    "    print(f\"Loaded {len(original_filtered_mentions)} filtered mentions from {filtered_mentions_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading filtered mentions JSON: {e}\")\n",
    "    original_filtered_mentions = [] # Ensure it's an empty list on error\n",
    "\n",
    "# --- Initialize NickNamer ---\n",
    "try:\n",
    "    nn = NickNamer()\n",
    "    print(\"NickNamer initialized.\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not initialize NickNamer. Nickname lookup disabled. Error: {e}\")\n",
    "    nn = None # Disable nickname lookup if initialization fails\n",
    "\n",
    "# --- Define Titles (for fallback/normalization if not a nickname) ---\n",
    "titles = {\"mr\", \"mrs\", \"miss\", \"ms\", \"dr\", \"lady\", \"sir\", \"colonel\", \"captain\", \"lord\"} # Lowercase\n",
    "\n",
    "# --- Consolidation Logic (Iterating through individual filtered mentions) ---\n",
    "# This dictionary will store aggregated data: {canonical_base: {\"count\": N, \"variations\": set()}}\n",
    "consolidated_characters = defaultdict(lambda: {\"count\": 0, \"variations\": set()})\n",
    "mention_texts_for_frequency = [] # Collect all original texts to get counts\n",
    "\n",
    "if original_filtered_mentions: # Check if the list is not empty\n",
    "    print(\"Starting character consolidation using NickNamer and rules...\")\n",
    "    for mention_data in original_filtered_mentions:\n",
    "        # Ensure mention_data is a dictionary with 'text' key\n",
    "        if not isinstance(mention_data, dict) or 'text' not in mention_data:\n",
    "             print(f\"Skipping invalid mention data: {mention_data}\")\n",
    "             continue\n",
    "\n",
    "        mention_text = mention_data['text'] # Use the cleaned text from filtering step\n",
    "        mention_texts_for_frequency.append(mention_text) # Store for counting frequency later\n",
    "        mention_lower = mention_text.lower()\n",
    "\n",
    "        canonical_base = None # This will hold the temporary grouping key\n",
    "\n",
    "        # --- Step 1: Check Nickname Dictionary ---\n",
    "        if nn: # Only if NickNamer initialized successfully\n",
    "            formal_names = nn.canonicals_of(mention_lower)\n",
    "            if formal_names:\n",
    "                # Simple strategy: use the first formal name found.\n",
    "                canonical_base = list(formal_names)[0]\n",
    "                # print(f\"DEBUG: Nickname mapping: '{mention_text}' -> '{canonical_base}'\") # Optional debug\n",
    "\n",
    "        # --- Step 2: Fallback to Rule-Based Normalization (if not found in nicknames) ---\n",
    "        if canonical_base is None:\n",
    "            parts = mention_text.split()\n",
    "            # Simple title stripping (if title is first word)\n",
    "            if parts and parts[0].lower().strip('.') in titles:\n",
    "                canonical_base = \" \".join(parts[1:]) # Use name after title\n",
    "            else:\n",
    "                canonical_base = mention_text # Use original (cleaned) text\n",
    "\n",
    "            # If after stripping title, the name is empty, use original text\n",
    "            if not canonical_base:\n",
    "                canonical_base = mention_text\n",
    "\n",
    "            # Normalize to lowercase for consistent grouping *before* final formatting\n",
    "            canonical_base = canonical_base.lower()\n",
    "\n",
    "        # --- Grouping ---\n",
    "        # Group based on the derived canonical_base (lowercase)\n",
    "        consolidated_characters[canonical_base][\"count\"] += 1\n",
    "        consolidated_characters[canonical_base][\"variations\"].add(mention_text) # Add the original mention text as a variation\n",
    "\n",
    "    # --- Step 3: Refine Canonical Keys and Final Formatting ---\n",
    "    print(\"Refining canonical keys...\")\n",
    "    final_consolidated_list = []\n",
    "    mention_counts = Counter(mention_texts_for_frequency) # Count frequencies of original mentions\n",
    "\n",
    "    for base_key, data in consolidated_characters.items():\n",
    "        # Choose the most frequent variation within the group as the final key representation\n",
    "        most_frequent_variation = base_key # Default to the base itself\n",
    "        max_freq = 0\n",
    "        for variation in data[\"variations\"]:\n",
    "            freq = mention_counts.get(variation, 0)\n",
    "            if freq > max_freq:\n",
    "                max_freq = freq\n",
    "                most_frequent_variation = variation\n",
    "            # Optional: Tie-breaking (e.g., prefer longer variation if frequencies are equal)\n",
    "            elif freq == max_freq and len(variation) > len(most_frequent_variation):\n",
    "                most_frequent_variation = variation\n",
    "\n",
    "        # Format the chosen key (e.g., Title_Case, Underscores)\n",
    "        parts = most_frequent_variation.split()\n",
    "        final_key_parts = []\n",
    "        title_prefix = \"\"\n",
    "        start_index = 0\n",
    "        if parts and parts[0].lower().strip('.') in titles:\n",
    "            title_prefix = parts[0].strip('.').capitalize() + \"_\"\n",
    "            start_index = 1\n",
    "        final_key_parts = [part.capitalize() for part in parts[start_index:]]\n",
    "        final_canonical_key = title_prefix + \"_\".join(final_key_parts)\n",
    "\n",
    "        # Fallback if key is empty\n",
    "        if not final_canonical_key:\n",
    "            final_canonical_key = f\"Unknown_{base_key[:10]}\"\n",
    "\n",
    "        # Append final data for this character group\n",
    "        final_consolidated_list.append({\n",
    "            \"canonical_key\": final_canonical_key,\n",
    "            \"total_mentions\": data[\"count\"],\n",
    "            \"variations\": sorted(list(data[\"variations\"])),\n",
    "            \"variation_count\": len(data[\"variations\"])\n",
    "        })\n",
    "\n",
    "    # --- Sort by total mentions (descending) ---\n",
    "    final_consolidated_list.sort(key=lambda x: x['total_mentions'], reverse=True)\n",
    "\n",
    "    # --- Save to CSV ---\n",
    "    \n",
    "    output_csv = \"../data/character_analysis_consolidated_nicknames.csv\"\n",
    "    try:\n",
    "        df_consolidated = pd.DataFrame(final_consolidated_list)\n",
    "        # making sure columns are in a sensible order\n",
    "        df_consolidated = df_consolidated[['canonical_key', 'total_mentions', 'variation_count', 'variations']]\n",
    "        df_consolidated.to_csv(output_csv, index=False)\n",
    "        print(f\"\\nSuccessfully saved consolidated results (nicknames + rules) to '{output_csv}'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError saving consolidated results: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nSkipping consolidation as no filtered PERSON mentions were loaded.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
