{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "681161c4",
   "metadata": {},
   "source": [
    "# Project Report: NLP Pipeline for Bechdel Test Analysis\n",
    "\n",
    "## 1. Context and Task Description\n",
    "\n",
    "### 1.1 Context\n",
    "The Bechdel-Wallace test is a measure of the representation of women in fiction. While simple, it provides a basic metric for assessing female presence and interaction. The test consists of three criteria:\n",
    "1.  The work must have at least two named female characters.\n",
    "2.  Who talk to each other.\n",
    "3.  About something besides a man.\n",
    "\n",
    "This project focuses on developing an automated Natural Language Processing (NLP) pipeline to determine if literary texts (specifically novels) pass the **first step** of the Bechdel-Wallace test: identifying whether the novel contains at least two named female characters.\n",
    "\n",
    "### 1.2 Task Description\n",
    "The primary task is to process the raw text of a novel and determine the number of unique, named female characters present. This involves several sub-tasks implemented within this NLP pipeline:\n",
    "\n",
    "1.  **Text Preprocessing:** Cleaning the raw novel text to prepare it for downstream analysis (e.g., removing headers/footers, normalizing quotes, fixing paragraph breaks).\n",
    "2.  **Character Identification:** Automatically identifying mentions of characters within the text using Named Entity Recognition (NER) and consolidating different references (names, titles, aliases, nicknames) to the same character.\n",
    "3.  **Gender Classification:** Assigning a gender (Male, Female, or Unknown) to each identified unique character using a combination of rule-based methods (titles, name lists) and context-based approaches (pronoun analysis, coreference resolution).\n",
    "4.  **Final Check:** Counting the number of characters classified as 'Female' by the pipeline to determine if the novel meets the first criterion of the Bechdel test.\n",
    "5.  **Evaluation:** Assessing the performance of the gender classification component against manually annotated ground truth data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864c5afd",
   "metadata": {},
   "source": [
    "## 2. Datasets Used\n",
    "\n",
    "### 2.1 Primary Dataset\n",
    "The primary data for this project consists of the full text of novels sourced from **Project Gutenberg**, which provides the raw textual content for analysis. \n",
    "\n",
    "**Specific Novels Used:**\n",
    "* **Dracula** by Bram Stoker\n",
    "* **Emma** by Jane Austen\n",
    "* **Pride and Prejudice** by Jane Austen\n",
    "\n",
    "These novels were selected because they offer diverse writing styles, time periods, and character distributions. All were sourced from Project Gutenberg, which provides free, public domain literary works.\n",
    "\n",
    "### 2.2 Evaluation Dataset\n",
    "For evaluation and grounding of the character/gender identification, **annotated books from the QuoteLi project** were used. These annotations provide ground truth information about characters and their genders.\n",
    "\n",
    "Link: [QuoteLi Project](https://muzny.github.io/quoteli.html)\n",
    "\n",
    "### 2.3 Additional Resources\n",
    "The project also utilizes name lists to support gender classification:\n",
    "* Lists of common male and female names (`female_names.txt` and `male_names.txt`)\n",
    "* These name lists contain approximately 200 names each and serve as a reference for the gender classification component\n",
    "\n",
    "### 2.4 Dataset Summary\n",
    "\n",
    "| Dataset Component | Description | Size | Source | Purpose |\n",
    "|-------------------|-------------|------|--------|---------|\n",
    "| Novel Texts | Full text of Dracula | ~848K chars | Project Gutenberg | Primary analysis text |\n",
    "| Novel Texts | Full text of Emma | ~885K chars | Project Gutenberg | Primary analysis text |\n",
    "| Novel Texts | Full text of Pride & Prejudice | ~723K chars | Project Gutenberg | Primary analysis text |\n",
    "| Female Names | Common female names | 203 names | Custom-created | Gender classification |\n",
    "| Male Names | Common male names | 200 names | Custom-created | Gender classification |\n",
    "| Ground Truth | Character gender annotations | 146 characters | QuoteLi project | Evaluation |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2b9c615",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Yassin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 1: Import libraries\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8e3df4",
   "metadata": {},
   "source": [
    "## 3. Pipeline Design and Implementation\n",
    "\n",
    "The pipeline consists of four main components, each implemented in a separate notebook:\n",
    "\n",
    "### 3.1 Text Preprocessing (`00_pre_proc.ipynb`)\n",
    "\n",
    "**Design Principles:**\n",
    "- Clean Project Gutenberg texts while preserving important textual features\n",
    "- Remove metadata, headers, footers, and illustrations\n",
    "- Normalize different types of quotation marks\n",
    "- Fix paragraph breaks to create proper paragraph structure\n",
    "\n",
    "**Implementation Details:**\n",
    "- Used regex patterns to identify and extract the main content from Project Gutenberg headers/footers\n",
    "- Normalized various quote styles (curly quotes, straight quotes) to standard format\n",
    "- Implemented paragraph fixing to handle hard-wrapped lines common in digitized texts\n",
    "- Removed illustration blocks that would interfere with entity recognition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9965bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Define functions\n",
    "def extract_gutenberg_content(text):\n",
    "    \"\"\"Extract the actual content of a Gutenberg book, removing headers and footers.\"\"\"\n",
    "    start_pattern = r\"\\*\\*\\* START OF (THIS|THE) PROJECT GUTENBERG EBOOK .+? \\*\\*\\*\"\n",
    "    end_pattern = r\"\\*\\*\\* END OF (THIS|THE) PROJECT GUTENBERG EBOOK .+? \\*\\*\\*\"\n",
    "    \n",
    "    start_match = re.search(start_pattern, text)\n",
    "    end_match = re.search(end_pattern, text)\n",
    "    \n",
    "    if start_match and end_match:\n",
    "        return text[start_match.end():end_match.start()].strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def normalize_quotes(text):\n",
    "    \"\"\"Normalize different kinds of quotes to standard single/double quotes.\"\"\"\n",
    "    # Replace various curly single quotes and backticks with standard apostrophe\n",
    "    text = re.sub(r\"[‘’‛`]\", \"'\", text)\n",
    "    # Replace various curly double quotes with standard double quote\n",
    "    text = re.sub(r'[“”„‟]', '\"', text)\n",
    "    return text\n",
    "\n",
    "import re\n",
    "\n",
    "def fix_paragraphs(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Join hard‑wrapped lines inside a paragraph while preserving true paragraph\n",
    "    breaks.  No external libraries required.\n",
    "    ──────────────────────────────────────────────────────────────────────────\n",
    "    Rule of thumb:\n",
    "      • If a line ends with . ! ? \" ’ ” )   ⇒ keep the break.\n",
    "      • Otherwise                           ⇒ join with the next line.\n",
    "    \"\"\"\n",
    "    out_lines = []\n",
    "    buffer    = []\n",
    "\n",
    "    for line in text.splitlines():\n",
    "        if not line.strip():               # blank ⇒ paragraph break\n",
    "            if buffer:\n",
    "                out_lines.append(\" \".join(buffer))\n",
    "                buffer.clear()\n",
    "            out_lines.append(\"\")           # keep one blank line\n",
    "            continue\n",
    "\n",
    "        buffer.append(line.strip())\n",
    "\n",
    "        # if this line *really* ends a sentence, flush the buffer\n",
    "        if re.search(r'[.!?][\"\\')\\]]?\\s*$', line):\n",
    "            out_lines.append(\" \".join(buffer))\n",
    "            buffer.clear()\n",
    "\n",
    "    if buffer:\n",
    "        out_lines.append(\" \".join(buffer))\n",
    "\n",
    "    return \"\\n\\n\".join(out_lines)\n",
    "\n",
    "\n",
    "\n",
    "def remove_illustrations(text):\n",
    "    \"\"\"\n",
    "    Removes multi-line illustration blocks starting with '[Illustration:' \n",
    "    and ending with ']'. Handles potential leading whitespace.\n",
    "    \"\"\"\n",
    "    # Regex explanation:\n",
    "    # ^\\s* : Matches the start of a line (^) followed by optional whitespace (\\s*)\n",
    "    # \\[Illustration: : Matches the literal starting text (square bracket escaped)\n",
    "    # .*?       : Matches any character (.), including newlines (due to re.DOTALL),\n",
    "    #             zero or more times (*), but as few times as possible (?) to stop at the first ']'\n",
    "    # \\]        : Matches the literal closing square bracket (escaped)\n",
    "    pattern = r\"^\\s*\\[Illustration:?.*?\\]\"\n",
    "    \n",
    "    # re.sub to replace found patterns with an empty string\n",
    "    cleaned_text = re.sub(pattern, \"\", text, flags=re.MULTILINE | re.DOTALL)\n",
    "    \n",
    "    # here I remove potentially resulting empty lines\n",
    "    cleaned_text = re.sub(r'\\n\\s*\\n', '\\n\\n', cleaned_text) # Replace lines containing only whitespace with a single blank line if desired\n",
    "    \n",
    "    return cleaned_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4bbbb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Basic Preprocessing for ../data/pp_novel.txt ---\n",
      "Loaded raw text (748126 chars).\n",
      "Length after Gutenberg extraction: 728713\n",
      "Length after removing illustrations: 721084\n",
      "Length after quote normalization: 721084\n",
      "Length after paragraph fixing: 723733\n",
      "\n",
      "Successfully saved cleaned text to: ../data/pp_cleaned.txt\n",
      "\n",
      "--- Basic Preprocessing finished for ../data/pp_novel.txt ---\n"
     ]
    }
   ],
   "source": [
    "#Cell 3 \n",
    "\n",
    "# --- Configuration ---\n",
    "input_file_path = \"../data/pp_novel.txt\" \n",
    "output_file_path = \"../data/pp_cleaned.txt\" \n",
    "\n",
    "# --- Workflow ---\n",
    "print(f\"--- Starting Basic Preprocessing for {input_file_path} ---\")\n",
    "processed_text = \"\" # Initialize variable\n",
    "try:\n",
    "    with open(input_file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        raw_text = file.read()\n",
    "    print(f\"Loaded raw text ({len(raw_text)} chars).\")\n",
    "\n",
    "    # Step 1: Initial Gutenberg Cleanup\n",
    "    # Input: raw_text\n",
    "    gutenberg_body = extract_gutenberg_content(raw_text)\n",
    "    print(f\"Length after Gutenberg extraction: {len(gutenberg_body)}\")\n",
    "    \n",
    "    # Step 2: Remove Illustrations\n",
    "    # Input: gutenberg_body\n",
    "    text_no_illustrations = remove_illustrations(gutenberg_body)\n",
    "    print(f\"Length after removing illustrations: {len(text_no_illustrations)}\")\n",
    "\n",
    "    # Step 3: Normalize Quotes\n",
    "    # Input: text_no_illustrations (output of Step 2)\n",
    "    text_norm_quotes = normalize_quotes(text_no_illustrations) \n",
    "    print(f\"Length after quote normalization: {len(text_norm_quotes)}\")\n",
    "     \n",
    "    # Step 4: Fix Paragraphs\n",
    "    # Input: text_norm_quotes (output of Step 3)\n",
    "    processed_text = fix_paragraphs(text_norm_quotes) \n",
    "    print(f\"Length after paragraph fixing: {len(processed_text)}\")\n",
    "\n",
    "    # Step 5: Save the final result\n",
    "    if processed_text:\n",
    "         try:\n",
    "            with open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "                outfile.write(processed_text)\n",
    "            print(f\"\\nSuccessfully saved cleaned text to: {output_file_path}\")\n",
    "         except Exception as e:\n",
    "            print(f\"\\nError saving file {output_file_path}: {e}\")\n",
    "    else:\n",
    "         print(\"\\nSkipping save as processed text is empty.\")\n",
    "\n",
    "    # debug: Print sample of final result\n",
    "    # if processed_text:\n",
    "    #     print(\"\\nSample of final processed text:\")\n",
    "    #     print(processed_text[:1000] + \"...\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Input file not found at {input_file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during processing: {e}\")\n",
    "\n",
    "print(f\"\\n--- Basic Preprocessing finished for {input_file_path} ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
