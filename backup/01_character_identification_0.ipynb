{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ef47ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import libraries\n",
    "import spacy\n",
    "import os \n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET # Added in the early steps for XML parsing\n",
    "\n",
    "print(\"Libraries imported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dc751b",
   "metadata": {},
   "source": [
    "## Load Cleaned Text\n",
    "Read the content of the cleaned text file created by `00_pre_proc.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44ef9f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded cleaned text from: ../data/pp_cleaned.txt\n",
      "Text length: 723733 characters\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load cleaned text\n",
    "input_file_path = \"../data/pp_cleaned.txt\" # Make sure this path is correct\n",
    "cleaned_text = None\n",
    "\n",
    "try:\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "        cleaned_text = file.read()\n",
    "    print(f\"Successfully loaded cleaned text from: {input_file_path}\")\n",
    "    print(f\"Text length: {len(cleaned_text)} characters\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Cleaned text file not found at {input_file_path}\")\n",
    "    print(\"Please ensure '00_pre_proc.ipynb' was run successfully and saved the file.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred loading the file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_spacy_md",
   "metadata": {},
   "source": [
    "## Load spaCy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4b923b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spaCy model 'en_core_web_trf'...\n",
      "spaCy model loaded.\n",
      "Pipeline components: ['transformer', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load spaCy model\n",
    "nlp = None\n",
    "if cleaned_text:\n",
    "    print(\"Loading spaCy model 'en_core_web_trf'...\")\n",
    "    try:\n",
    "        # Make sure you have downloaded the model: python -m spacy download en_core_web_lg\n",
    "        nlp = spacy.load(\"en_core_web_trf\") \n",
    "        print(\"spaCy model loaded.\")\n",
    "        print(\"Pipeline components:\", nlp.pipe_names) # Should show 'ner' among others\n",
    "    except OSError:\n",
    "        print(\"Error: spaCy model 'en_core_web_lg' not found.\")\n",
    "        print(\"Download it by running: python -m spacy download en_core_web_lg\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred loading the spaCy model: {e}\")\n",
    "else:\n",
    "    print(\"Skipping spaCy model loading as cleaned_text is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "process_text_md",
   "metadata": {},
   "source": [
    "## Process Text for Named Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "380577ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text with spaCy NER pipeline (this may take some time)...\n",
      "Text processing complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Process text with spaCy NER pipeline\n",
    "doc = None\n",
    "if cleaned_text and nlp:\n",
    "     print(\"Processing text with spaCy NER pipeline (this may take some time)...\")\n",
    "     # Increase max_length if needed, but be mindful of memory usage\n",
    "     # nlp.max_length = len(cleaned_text) + 100 \n",
    "     try:\n",
    "         doc = nlp(cleaned_text)\n",
    "         print(\"Text processing complete.\")\n",
    "     except ValueError as ve:\n",
    "         print(f\"ValueError during processing: {ve}\")\n",
    "         print(\"The text might be too long for the default spaCy model settings.\")\n",
    "         print(\"Consider increasing nlp.max_length or processing in chunks.\")\n",
    "     except Exception as e:\n",
    "         print(f\"An unexpected error occurred during text processing: {e}\")\n",
    "else:\n",
    "     print(\"Skipping text processing as cleaned_text or nlp model is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extract_persons_md",
   "metadata": {},
   "source": [
    "## Extract PERSON Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fafdecd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Extracting PERSON Entities ---\n",
      "Found 3768 PERSON mentions.\n",
      "First 10 mentions: [{'text': 'Jane Austen', 'start_char': 28, 'end_char': 39}, {'text': 'George Saintsbury', 'start_char': 62, 'end_char': 79}, {'text': 'Hugh Thomson\\n\\n\\n\\nRuskin', 'start_char': 101, 'end_char': 123}, {'text': 'George Allen', 'start_char': 179, 'end_char': 191}, {'text': 'Walt Whitman', 'start_char': 294, 'end_char': 306}, {'text': 'Edmund', 'start_char': 1621, 'end_char': 1627}, {'text': 'Fanny', 'start_char': 1638, 'end_char': 1643}, {'text': 'Mary', 'start_char': 1652, 'end_char': 1656}, {'text': 'Fanny', 'start_char': 1679, 'end_char': 1684}, {'text': 'Crawford', 'start_char': 1714, 'end_char': 1722}]\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Extract PERSON entities found by NER\n",
    "person_mentions = []\n",
    "if doc:\n",
    "    print(\"\\n--- Extracting PERSON Entities ---\")\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            # Store text, start/end character offsets\n",
    "            person_mentions.append({\n",
    "                \"text\": ent.text,\n",
    "                \"start_char\": ent.start_char,\n",
    "                \"end_char\": ent.end_char\n",
    "            })\n",
    "    print(f\"Found {len(person_mentions)} PERSON mentions.\")\n",
    "    # Optional: Print first few mentions\n",
    "    if person_mentions:\n",
    "         print(\"First 10 mentions:\", person_mentions[:10])\n",
    "else:\n",
    "     print(\"Skipping extraction as doc object is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d92e9ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving extracted PERSON mentions to ../data/ner_person_mentions.json...\n",
      "Raw NER results saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Save raw PERSON mentions (optional but good for debugging)\n",
    "if person_mentions:\n",
    "    output_data_path = \"../data/ner_person_mentions.json\" # Choose your output format/name\n",
    "\n",
    "    print(f\"\\nSaving extracted PERSON mentions to {output_data_path}...\")\n",
    "    try:\n",
    "        # Ensure data directory exists\n",
    "        os.makedirs(os.path.dirname(output_data_path), exist_ok=True)\n",
    "        with open(output_data_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(person_mentions, f, ensure_ascii=False, indent=4)\n",
    "        print(\"Raw NER results saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving NER results: {e}\")\n",
    "else:\n",
    "    print(\"\\nNo PERSON mentions extracted to save.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_aliases_md",
   "metadata": {},
   "source": [
    "## Load Aliases from Annotated XML\n",
    "\n",
    "To improve character consolidation (e.g., grouping 'Lizzy' with 'Elizabeth'), we load the character list and aliases from the annotated XML file (`pp_full.xml`). This acts as a lookup table based on our 'training' data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "load_aliases_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading aliases from ../data/pp_full.xml...\n",
      "Created alias map with 100 entries.\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Load aliases from XML\n",
    "xml_file_path = \"../data/pp_full.xml\" # Path to the annotated file\n",
    "alias_to_canonical = {}\n",
    "canonical_name_gender = {} # Store gender info from XML as well\n",
    "\n",
    "print(f\"Loading aliases from {xml_file_path}...\")\n",
    "try:\n",
    "    tree = ET.parse(xml_file_path)\n",
    "    root = tree.getroot()\n",
    "    characters_element = root.find('characters')\n",
    "    \n",
    "    if characters_element is not None:\n",
    "        for character in characters_element.findall('character'):\n",
    "            canonical_name = character.get('name')\n",
    "            aliases_str = character.get('aliases')\n",
    "            gender = character.get('gender') # Get gender attribute\n",
    "            \n",
    "            if not canonical_name:\n",
    "                continue # Skip if character has no canonical name\n",
    "                \n",
    "            # Store gender for the canonical name\n",
    "            if gender:\n",
    "                canonical_name_gender[canonical_name] = gender.capitalize()\n",
    "            \n",
    "            # Add canonical name itself to map (normalized)\n",
    "            normalized_canonical = canonical_name.lower().strip()\n",
    "            if normalized_canonical:\n",
    "                 alias_to_canonical[normalized_canonical] = canonical_name\n",
    "            \n",
    "            # Add aliases to map (normalized)\n",
    "            if aliases_str:\n",
    "                aliases = aliases_str.split(';')\n",
    "                for alias in aliases:\n",
    "                    normalized_alias = alias.lower().strip()\n",
    "                    if normalized_alias:\n",
    "                        # Simple approach: map alias to canonical name\n",
    "                        # More complex: handle ambiguous aliases like 'Miss Bennet' if needed\n",
    "                        if normalized_alias not in alias_to_canonical: # Avoid overwriting if already mapped\n",
    "                             alias_to_canonical[normalized_alias] = canonical_name\n",
    "                        # else: print(f\"Ambiguous or duplicate alias ignored: {normalized_alias}\")\n",
    "                             \n",
    "        print(f\"Created alias map with {len(alias_to_canonical)} entries.\")\n",
    "        # print(\"Sample alias map:\", dict(list(alias_to_canonical.items())[:15]))\n",
    "        # print(\"Canonical genders:\", canonical_name_gender)\n",
    "    else:\n",
    "        print(\"Error: <characters> tag not found in XML.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: XML file not found at {xml_file_path}\")\n",
    "except ET.ParseError as pe:\n",
    "    print(f\"Error parsing XML file {xml_file_path}: {pe}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred loading aliases: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidate_chars_md",
   "metadata": {},
   "source": [
    "## Consolidate Characters using Alias Map\n",
    "\n",
    "Now, group the PERSON mentions found by NER using the alias map. If a mention matches an alias, group it under the canonical name. Otherwise, use fallback logic (e.g., normalized name or surname)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "consolidate_chars_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidating mentions using alias map...\n",
      "Consolidation complete. Processed 3768 mentions.\n",
      "Mapped 2827 mentions using aliases.\n",
      "Resulting unique character keys: 163\n",
      "\n",
      "Top characters after consolidation:\n",
      "        canonical_key  total_mentions  \\\n",
      "26   Elizabeth_Bennet             755   \n",
      "25           Mr_Darcy             422   \n",
      "36         Mrs_Bennet             319   \n",
      "19         Mr_Bingley             306   \n",
      "18        Jane_Bennet             295   \n",
      "16         Mr_Wickham             198   \n",
      "21            Collins             189   \n",
      "15       Lydia_Bennet             175   \n",
      "49     Lady_Catherine             116   \n",
      "102          Gardiner              94   \n",
      "58          Charlotte              87   \n",
      "50       Kitty_Bennet              71   \n",
      "79              Lucas              44   \n",
      "7         Mary_Bennet              39   \n",
      "89            Forster              39   \n",
      "125       Fitzwilliam              35   \n",
      "52            Philips              34   \n",
      "80              Hurst              33   \n",
      "78            William              32   \n",
      "11             Austen              25   \n",
      "\n",
      "                                            variations  variation_count  \\\n",
      "26   Eliza, Eliza Bennet, Elizabeth, Elizabeth Benn...                5   \n",
      "25                            Darcy, Fitzwilliam Darcy                2   \n",
      "36                                              Bennet                1   \n",
      "19                                             Bingley                1   \n",
      "18                                                Jane                1   \n",
      "16                     George, George Wickham, Wickham                3   \n",
      "21                                             Collins                1   \n",
      "15                   LYDIA BENNET, Lydia, Lydia Bennet                3   \n",
      "49                                           Catherine                1   \n",
      "102                                           Gardiner                1   \n",
      "58                          Charlotte, Charlotte Lucas                2   \n",
      "50                                               Kitty                1   \n",
      "79                                               Lucas                1   \n",
      "7                                                 Mary                1   \n",
      "89                                             Forster                1   \n",
      "125                                        Fitzwilliam                1   \n",
      "52                                             Philips                1   \n",
      "80                                               Hurst                1   \n",
      "78                                             William                1   \n",
      "11                                              Austen                1   \n",
      "\n",
      "    gender_from_xml  \n",
      "26           Female  \n",
      "25             Male  \n",
      "36           Female  \n",
      "19             Male  \n",
      "18           Female  \n",
      "16             Male  \n",
      "21             None  \n",
      "15           Female  \n",
      "49           Female  \n",
      "102            None  \n",
      "58           Female  \n",
      "50           Female  \n",
      "79             None  \n",
      "7            Female  \n",
      "89             None  \n",
      "125            None  \n",
      "52             None  \n",
      "80             None  \n",
      "78             None  \n",
      "11             None  \n",
      "\n",
      "Consolidated character analysis data saved to '../data/character_analysis_consolidated.csv' and '../data/character_groups_consolidated.json'\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Consolidate character mentions using the alias map\n",
    "\n",
    "character_groups = {}\n",
    "mentions_processed = 0\n",
    "mentions_mapped = 0\n",
    "\n",
    "if person_mentions: # Check if NER ran successfully\n",
    "    print(\"Consolidating mentions using alias map...\")\n",
    "    for mention in person_mentions:\n",
    "        mentions_processed += 1\n",
    "        original_mention_text = mention['text']\n",
    "        normalized_mention = original_mention_text.lower().strip()\n",
    "        \n",
    "        # Attempt to find canonical name using the alias map\n",
    "        canonical_name = alias_to_canonical.get(normalized_mention)\n",
    "        \n",
    "        char_key = None\n",
    "        if canonical_name:\n",
    "            # Found in alias map, use the canonical name as the key\n",
    "            char_key = canonical_name\n",
    "            mentions_mapped += 1\n",
    "        else:\n",
    "            # Fallback: Use the original mention text as key for now\n",
    "            # More sophisticated fallback (like surname grouping) could be added here\n",
    "            # We also might want to filter out non-names like 'Meryton' here\n",
    "            # Simple filter: skip if it looks like a place (e.g., capitalized, maybe check against known places)\n",
    "            # For now, let's just use the original text as key if no alias match\n",
    "            char_key = original_mention_text \n",
    "            # Basic check to avoid adding obvious non-names if desired\n",
    "            if char_key in ['Meryton', 'London', 'Hertfordshire', 'Kent', 'Derbyshire']: # Example filter\n",
    "                 continue\n",
    "            \n",
    "        # Add to groups\n",
    "        if char_key not in character_groups:\n",
    "            character_groups[char_key] = {\n",
    "                'variations': set(),\n",
    "                'count': 0\n",
    "            }\n",
    "        \n",
    "        character_groups[char_key]['variations'].add(original_mention_text)\n",
    "        character_groups[char_key]['count'] += 1\n",
    "        \n",
    "    print(f\"Consolidation complete. Processed {mentions_processed} mentions.\")\n",
    "    print(f\"Mapped {mentions_mapped} mentions using aliases.\")\n",
    "    print(f\"Resulting unique character keys: {len(character_groups)}\")\n",
    "\n",
    "    # --- Create DataFrame for analysis ---\n",
    "    character_data_list = []\n",
    "    for key, data in character_groups.items():\n",
    "         # Get gender from XML if available for this canonical key\n",
    "         xml_gender = canonical_name_gender.get(key, None) \n",
    "         character_data_list.append({\n",
    "            'canonical_key': key, # Changed 'key' to 'canonical_key'\n",
    "            'total_mentions': data['count'],\n",
    "            'variations': ', '.join(sorted(list(data['variations']))), # Sort variations for consistency\n",
    "            'variation_count': len(data['variations']),\n",
    "            'gender_from_xml': xml_gender # Add gender from XML if found\n",
    "        })\n",
    "        \n",
    "    character_df = pd.DataFrame(character_data_list)\n",
    "\n",
    "    # Sort by total mentions to see main characters\n",
    "    character_df = character_df.sort_values('total_mentions', ascending=False)\n",
    "\n",
    "    print(\"\\nTop characters after consolidation:\")\n",
    "    print(character_df.head(20))\n",
    "\n",
    "    # --- Save the structured character data ---\n",
    "    output_csv = '../data/character_analysis_consolidated.csv'\n",
    "    output_json = '../data/character_groups_consolidated.json'\n",
    "    \n",
    "    try:\n",
    "        character_df.to_csv(output_csv, index=False)\n",
    "        # Save JSON compatible format\n",
    "        save_groups = {k: {'variations': list(v['variations']), 'count': v['count']} \n",
    "                       for k, v in character_groups.items()}\n",
    "        with open(output_json, 'w', encoding='utf-8') as f:\n",
    "            json.dump(save_groups, f, indent=4, ensure_ascii=False)\n",
    "        print(f\"\\nConsolidated character analysis data saved to '{output_csv}' and '{output_json}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError saving consolidated results: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"\\nSkipping consolidation as no PERSON mentions were extracted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next_steps_consolidation_md",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1.  **Review Output:** Check the new `character_analysis_consolidated.csv`. Does it correctly group 'Lizzy' under 'Elizabeth_Bennet'? Are other characters consolidated better?\n",
    "2.  **Refine Alias Map/Fallback:** You might need to refine how ambiguous aliases (like 'Miss Bennet') are handled or improve the fallback logic for mentions not in the alias map.\n",
    "3.  **Proceed to Gender Classification:** Now you can run the `02_gender_classification.ipynb` notebook, making sure it loads this *new* consolidated CSV (`character_analysis_consolidated.csv`). The gender classification should now operate on the canonical keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d459766",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f77e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import coreferee # Import coreferee\n",
    "import pandas as pd\n",
    "\n",
    "# --- Configuration ---\n",
    "GENDERED_CONTEXT_CSV_PATH = \"../data/character_analysis_gendered_contextual.csv\" # Input from Notebook 02b\n",
    "CLEANED_TEXT_PATH = \"../data/pp_cleaned.txt\" # Input from Notebook 00\n",
    "OUTPUT_CSV_PATH = \"../data/character_analysis_gendered_coref.csv\"\n",
    "GENDER_FEMALE = \"Female\"\n",
    "GENDER_MALE = \"Male\"\n",
    "GENDER_UNKNOWN = \"Unknown\"\n",
    "MALE_PRONOUNS = {'he', 'him', 'his'}\n",
    "FEMALE_PRONOUNS = {'she', 'her', 'hers'}\n",
    "\n",
    "# --- Load spaCy model and add coreferee ---\n",
    "print(\"Loading spaCy model and adding coreferee...\")\n",
    "# Load your spaCy model (make sure it's compatible with coreferee)\n",
    "nlp = spacy.load('en_core_web_trf')\n",
    "# Add the coreferee pipe\n",
    "# Coreferee automatically initializes when added if needed.\n",
    "nlp.add_pipe('coreferee')\n",
    "print(\"Pipeline:\", nlp.pipe_names)\n",
    "\n",
    "# --- Load Data ---\n",
    "print(f\"Loading data from {GENDERED_CONTEXT_CSV_PATH}...\")\n",
    "try:\n",
    "    char_df = pd.read_csv(GENDERED_CONTEXT_CSV_PATH)\n",
    "    print(f\"Loaded {len(char_df)} characters.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading CSV: {e}\")\n",
    "    char_df = None\n",
    "\n",
    "print(f\"Loading text from {CLEANED_TEXT_PATH}...\")\n",
    "try:\n",
    "    with open(CLEANED_TEXT_PATH, 'r', encoding='utf-8') as f:\n",
    "        full_text = f.read()\n",
    "    print(f\"Loaded text ({len(full_text)} chars).\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading text: {e}\")\n",
    "    full_text = None\n",
    "\n",
    "# --- Process the full text ---\n",
    "doc = None\n",
    "if full_text:\n",
    "    print(\"Processing text with spaCy and coreferee (this can take time)...\")\n",
    "    # Increase max_length if your text is very long\n",
    "    # nlp.max_length = len(full_text) + 100\n",
    "    doc = nlp(full_text)\n",
    "    print(\"Text processing complete.\")\n",
    "    # --- Access Coreference Chains ---\n",
    "    if doc._.coref_chains:\n",
    "         print(f\"Found {len(doc._.coref_chains)} coreference chains.\")\n",
    "         # Example: Print the first few chains\n",
    "         # doc._.coref_chains.print() # coreferee has a built-in print method\n",
    "    else:\n",
    "         print(\"No coreference chains found by coreferee.\")\n",
    "\n",
    "# --- Apply Coref Results to Gender Classification (Conceptual) ---\n",
    "if char_df is not None and doc is not None and doc._.coref_chains:\n",
    "    print(\"Applying coreference results to Unknown characters...\")\n",
    "    # Create a map from mention spans (start_token_index) to their chain index\n",
    "    mention_to_chain_index = {}\n",
    "    for chain_index, chain in enumerate(doc._.coref_chains):\n",
    "        for mention in chain:\n",
    "             # A mention in coreferee is a list of token indices\n",
    "             start_token_index = mention.token_indices[0]\n",
    "             mention_to_chain_index[start_token_index] = chain_index\n",
    "\n",
    "    # Create a map to store aggregated gender evidence per chain\n",
    "    chain_gender_evidence = {i: {'male': 0, 'female': 0, 'known_gender': GENDER_UNKNOWN} for i in range(len(doc._.coref_chains))}\n",
    "\n",
    "    # --- Pass 1: Populate evidence from known-gender mentions in chains ---\n",
    "    # (Requires mapping your classified characters back to tokens/mentions - complex step)\n",
    "    # Placeholder logic: Iterate through chains, check if any mention text matches a known M/F character variation\n",
    "    # If a chain contains \"Elizabeth\", mark chain_gender_evidence[chain_idx]['known_gender'] = GENDER_FEMALE\n",
    "    # If a chain contains \"Mr. Darcy\", mark chain_gender_evidence[chain_idx]['known_gender'] = GENDER_MALE\n",
    "    # Also count pronouns within the chain's mentions\n",
    "    for chain_index, chain in enumerate(doc._.coref_chains):\n",
    "        for mention in chain:\n",
    "             mention_text = doc[mention.token_indices[0]:mention.token_indices[-1]+1].text.lower()\n",
    "             # Check against known gendered characters (you'd need your previous results here)\n",
    "             # if mention_text corresponds to known Male char: chain_gender_evidence[chain_index]['known_gender'] = GENDER_MALE; break\n",
    "             # if mention_text corresponds to known Female char: chain_gender_evidence[chain_index]['known_gender'] = GENDER_FEMALE; break\n",
    "             # Count pronouns\n",
    "             if mention_text in MALE_PRONOUNS: chain_gender_evidence[chain_index]['male'] += 1\n",
    "             if mention_text in FEMALE_PRONOUNS: chain_gender_evidence[chain_index]['female'] += 1\n",
    "\n",
    "    # --- Pass 2: Classify 'Unknown' characters based on their chain's evidence ---\n",
    "    char_df['coref_gender'] = char_df['final_gender_contextual'] # Start with previous best guess\n",
    "    unknown_indices = char_df[char_df['coref_gender'] == GENDER_UNKNOWN].index\n",
    "\n",
    "    for index in unknown_indices:\n",
    "        char_name = char_df.loc[index, 'canonical_key']\n",
    "        variations = set(var.strip() for var in str(char_df.loc[index, 'variations']).split(','))\n",
    "        variations.add(char_name)\n",
    "\n",
    "        # Find mentions of this character in the doc\n",
    "        found_chain_indices = set()\n",
    "        # This part is tricky: Need to map character name back to mentions found by coreferee\n",
    "        # A simple approach: iterate through all mentions in all chains\n",
    "        for chain_index, chain in enumerate(doc._.coref_chains):\n",
    "             for mention in chain:\n",
    "                 mention_span = doc[mention.token_indices[0]:mention.token_indices[-1]+1]\n",
    "                 if mention_span.text in variations:\n",
    "                     found_chain_indices.add(chain_index)\n",
    "                     break # Found this char in this chain\n",
    "\n",
    "        # Aggregate evidence from all chains this character belongs to\n",
    "        final_male = 0\n",
    "        final_female = 0\n",
    "        final_known = GENDER_UNKNOWN\n",
    "        for chain_idx in found_chain_indices:\n",
    "             evidence = chain_gender_evidence[chain_idx]\n",
    "             final_male += evidence['male']\n",
    "             final_female += evidence['female']\n",
    "             if evidence['known_gender'] != GENDER_UNKNOWN:\n",
    "                  # If conflicting known genders in different chains, maybe mark ambiguous?\n",
    "                  final_known = evidence['known_gender'] # Simplistic: take last known\n",
    "\n",
    "        # Apply classification logic based on aggregated evidence\n",
    "        new_gender = GENDER_UNKNOWN\n",
    "        if final_known != GENDER_UNKNOWN:\n",
    "             new_gender = final_known\n",
    "        elif final_male > final_female: # Add thresholds if needed\n",
    "             new_gender = GENDER_MALE\n",
    "        elif final_female > final_male:\n",
    "             new_gender = GENDER_FEMALE\n",
    "\n",
    "        char_df.loc[index, 'coref_gender'] = new_gender\n",
    "\n",
    "    # --- Display/Save Results ---\n",
    "    print(\"\\\\nCharacters re-classified using Coreference:\")\n",
    "    # Show changes...\n",
    "    print(char_df[['canonical_key', 'final_gender_contextual', 'coref_gender']].head(20))\n",
    "    # Save char_df[['...','coref_gender']] to OUTPUT_CSV_PATH\n",
    "\n",
    "else:\n",
    "    print(\"Skipping coreference application due to missing data or coref chains.\")\n",
    "\n",
    "print(\"\\\\n--- Coreference Gender Classification Attempt Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70583a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
