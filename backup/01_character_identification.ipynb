{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ef47ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import libraries\n",
    "import spacy\n",
    "import os \n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET # Added for XML parsing\n",
    "\n",
    "print(\"Libraries imported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dc751b",
   "metadata": {},
   "source": [
    "## Load Cleaned Text\n",
    "Read the content of the cleaned text file created by `00_pre_proc.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44ef9f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded cleaned text from: ../data/pp_cleaned.txt\n",
      "Text length: 723733 characters\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load cleaned text\n",
    "input_file_path = \"../data/pp_cleaned.txt\" # Make sure this path is correct\n",
    "cleaned_text = None\n",
    "\n",
    "try:\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "        cleaned_text = file.read()\n",
    "    print(f\"Successfully loaded cleaned text from: {input_file_path}\")\n",
    "    print(f\"Text length: {len(cleaned_text)} characters\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Cleaned text file not found at {input_file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred loading the file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_spacy_md",
   "metadata": {},
   "source": [
    "## Load spaCy Model - First attempt with _lg then _trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b923b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load spaCy model\n",
    "nlp = None\n",
    "if cleaned_text:\n",
    "    try:\n",
    "        # Make sure you have downloaded the model: python -m spacy download en_core_web_lg\n",
    "        nlp = spacy.load(\"en_core_web_trf\") \n",
    "        print(\"spaCy model loaded.\")\n",
    "        print(\"Pipeline components:\", nlp.pipe_names) # Should show 'ner' among others\n",
    "    except OSError:\n",
    "        print(\"Error: spaCy model not found.\")\n",
    "        print(\"Download it by running: python -m spacy download name\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred loading the spaCy model: {e}\")\n",
    "else:\n",
    "    print(\"Skipping spaCy model loading as cleaned_text is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380577ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Process text with spaCy NER pipeline\n",
    "doc = None\n",
    "if cleaned_text and nlp:\n",
    "     print(\"Processing text with spaCy NER pipeline (this may take some time)...\")\n",
    "     # Increase max_length if needed, but be mindful of memory usage\n",
    "     # nlp.max_length = len(cleaned_text) + 100 \n",
    "     try:\n",
    "         doc = nlp(cleaned_text)\n",
    "         print(\"Text processing complete.\")\n",
    "     except ValueError as ve:\n",
    "         print(f\"ValueError during processing: {ve}\")\n",
    "         print(\"The text might be too long for the default spaCy model settings.\")\n",
    "         print(\"Consider increasing nlp.max_length or processing in chunks.\")\n",
    "     except Exception as e:\n",
    "         print(f\"An unexpected error occurred during text processing: {e}\")\n",
    "else:\n",
    "     print(\"Skipping text processing as cleaned_text or nlp model is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "process_text_md",
   "metadata": {},
   "source": [
    "## Process Text for Named Entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extract_persons_md",
   "metadata": {},
   "source": [
    "## Extract PERSON Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafdecd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Extract PERSON entities found by NER\n",
    "person_mentions = []\n",
    "if doc:\n",
    "    print(\"\\n--- Extracting PERSON Entities ---\")\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            # Store text, start/end character offsets\n",
    "            person_mentions.append({\n",
    "                \"text\": ent.text,\n",
    "                \"start_char\": ent.start_char,\n",
    "                \"end_char\": ent.end_char\n",
    "            })\n",
    "    print(f\"Found {len(person_mentions)} PERSON mentions.\")\n",
    "    # Optional: Print first few mentions\n",
    "    if person_mentions:\n",
    "         print(\"First 10 mentions:\", person_mentions[:10])\n",
    "else:\n",
    "     print(\"Skipping extraction as doc object is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d92e9ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 6: Save raw PERSON mentions (optional but good for debugging)\n",
    "if person_mentions:\n",
    "    output_data_path = \"../data/ner_person_mentions_trf.json\" # Choose your output format/name\n",
    "\n",
    "    print(f\"\\nSaving extracted PERSON mentions to {output_data_path}...\")\n",
    "    try:\n",
    "        # Ensure data directory exists\n",
    "        os.makedirs(os.path.dirname(output_data_path), exist_ok=True)\n",
    "        with open(output_data_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(person_mentions, f, ensure_ascii=False, indent=4)\n",
    "        print(\"Raw NER results saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving NER results: {e}\")\n",
    "else:\n",
    "    print(\"\\nNo PERSON mentions extracted to save.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84777f81",
   "metadata": {},
   "source": [
    "## We should improve NER - Second attempt way better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3dd5db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Starting chunk processing. Total text length: 723733\n",
      "DEBUG: Chunk processing finished.\n",
      "DEBUG: Total PERSON mentions found across all chunks: 5498\n",
      "DEBUG: Total LOCATION mentions found across all chunks: 538\n",
      "DEBUG: Unique PERSON mentions after deduplication: 5198\n",
      "Successfully saved 5198 person mentions to ../data/ner_person_mentions_bert.json\n",
      "\n",
      "First few person mentions:\n",
      "{'text': 'Jane Austen', 'start_char': 28, 'end_char': 39}\n",
      "{'text': 'George Saintsbury', 'start_char': 62, 'end_char': 79}\n",
      "{'text': 'Hugh Thomson Ruskin', 'start_char': 101, 'end_char': 123}\n",
      "{'text': 'George Allen', 'start_char': 179, 'end_char': 191}\n",
      "{'text': 'Walt Whitman', 'start_char': 294, 'end_char': 306}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# --- Load your full cleaned_text variable ---\n",
    "# (Make sure this is the complete text)\n",
    "# cleaned_text = ...\n",
    "\n",
    "# --- Initialize the pipeline ---\n",
    "literary_ner = pipeline(\"ner\", model=\"compnet-renard/bert-base-cased-literary-NER\", aggregation_strategy=\"first\")\n",
    "\n",
    "# --- Define Chunking Parameters (Example) ---\n",
    "# You'll need to determine good values based on model limits (e.g., target ~400-500 tokens)\n",
    "# For simplicity, let's use character count here, but token count is better.\n",
    "chunk_size = 2000 # Number of characters per chunk (adjust!)\n",
    "overlap = 200    # Number of characters overlap (adjust!)\n",
    "\n",
    "# --- Process in Chunks ---\n",
    "all_person_mentions = []\n",
    "all_location_mentions = []\n",
    "current_pos = 0\n",
    "\n",
    "print(f\"DEBUG: Starting chunk processing. Total text length: {len(cleaned_text)}\")\n",
    "\n",
    "while current_pos < len(cleaned_text):\n",
    "    chunk_start = current_pos\n",
    "    chunk_end = min(current_pos + chunk_size, len(cleaned_text))\n",
    "    text_chunk = cleaned_text[chunk_start:chunk_end]\n",
    "\n",
    "    # print(f\"DEBUG: Processing chunk: {chunk_start} - {chunk_end}\") # Optional debug\n",
    "\n",
    "    if not text_chunk.strip(): # Skip empty chunks if any\n",
    "        current_pos += chunk_size - overlap\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "         # Run NER on the chunk\n",
    "         ner_results_chunk = literary_ner(text_chunk)\n",
    "\n",
    "         # Process results for this chunk\n",
    "         for entity in ner_results_chunk:\n",
    "             # Adjust character offsets to be relative to the full text\n",
    "             original_start = chunk_start + entity['start']\n",
    "             original_end = chunk_start + entity['end']\n",
    "\n",
    "             mention_data = {\n",
    "                 \"text\": entity['word'],\n",
    "                 \"start_char\": original_start,\n",
    "                 \"end_char\": original_end\n",
    "                 # Add score if needed: \"score\": entity['score']\n",
    "             }\n",
    "\n",
    "             if entity.get('entity_group') == 'PER':\n",
    "                 all_person_mentions.append(mention_data)\n",
    "             elif entity.get('entity_group') == 'LOC':\n",
    "                 all_location_mentions.append(mention_data)\n",
    "\n",
    "    except Exception as e:\n",
    "         print(f\"ERROR processing chunk {chunk_start}-{chunk_end}: {e}\") # Log errors\n",
    "\n",
    "    # Move to the next chunk position\n",
    "    # If it's the last chunk, stop\n",
    "    if chunk_end == len(cleaned_text):\n",
    "        break\n",
    "    current_pos += chunk_size - overlap # Move forward, maintaining overlap\n",
    "\n",
    "\n",
    "print(f\"DEBUG: Chunk processing finished.\")\n",
    "print(f\"DEBUG: Total PERSON mentions found across all chunks: {len(all_person_mentions)}\")\n",
    "print(f\"DEBUG: Total LOCATION mentions found across all chunks: {len(all_location_mentions)}\")\n",
    "\n",
    "# --- Remove duplicates ---\n",
    "# Entities might be detected in overlapping regions, need deduplication\n",
    "# Simple deduplication based on exact start/end/text match\n",
    "unique_person_mentions_set = set()\n",
    "unique_person_mentions = []\n",
    "for mention in all_person_mentions:\n",
    "    mention_tuple = (mention['text'], mention['start_char'], mention['end_char'])\n",
    "    if mention_tuple not in unique_person_mentions_set:\n",
    "        unique_person_mentions_set.add(mention_tuple)\n",
    "        unique_person_mentions.append(mention)\n",
    "\n",
    "print(f\"DEBUG: Unique PERSON mentions after deduplication: {len(unique_person_mentions)}\")\n",
    "\n",
    "\n",
    "# --- Now use unique_person_mentions for filtering, saving, consolidation ---\n",
    "person_mentions = unique_person_mentions # Assign to the variable name used later\n",
    "\n",
    "# After your NER processing and deduplication...\n",
    "if person_mentions:\n",
    "    import json\n",
    "    output_data_path = \"../data/ner_person_mentions_bert.json\"\n",
    "    \n",
    "    # Save to JSON file\n",
    "    try:\n",
    "        with open(output_data_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(person_mentions, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"Successfully saved {len(person_mentions)} person mentions to {output_data_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving to JSON: {e}\")\n",
    "else:\n",
    "    print(\"No person mentions found to save\")\n",
    "\n",
    "# Optional: Print first few entries to verify the data\n",
    "print(\"\\nFirst few person mentions:\")\n",
    "for mention in person_mentions[:5]:\n",
    "    print(mention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a68790",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b79ac5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Starting Post-NER Filtering on 5198 mentions...\n",
      "DEBUG: Mentions remaining after filtering: 3986\n",
      "DEBUG: Saving 3986 filtered mentions to ../data/ner_person_mentions_bert_filtered.json...\n",
      "DEBUG: Successfully saved filtered mentions.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# List of standalone titles/honorifics to filter if they appear alone\n",
    "standalone_titles = {\"mr\", \"mrs\", \"miss\", \"ms\", \"dr\", \"lady\", \"sir\", \"colonel\", \"captain\", \"lord\"} # Add more\n",
    "# Characters considered punctuation for stripping/checking\n",
    "import string\n",
    "punctuation_chars = string.punctuation # Gets '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "\n",
    "# --- Implement Post-NER Filtering ---\n",
    "filtered_person_mentions = []\n",
    "if person_mentions: # Use the list generated by the HF model\n",
    "    print(f\"DEBUG: Starting Post-NER Filtering on {len(person_mentions)} mentions...\")\n",
    "    for mention in person_mentions:\n",
    "        original_text = mention['text']\n",
    "\n",
    "        # 1. Basic Cleaning: Remove leading/trailing whitespace and punctuation\n",
    "        #    Example: \". Bennet \" -> \"Bennet\" ; \".\" -> \"\" ; \"Mr.\" -> \"Mr\"\n",
    "        cleaned_text = original_text.strip().strip(punctuation_chars)\n",
    "        cleaned_text_lower = cleaned_text.lower()\n",
    "\n",
    "        # --- Apply Filters ---\n",
    "        # Filter 1: Check if empty after cleaning (e.g., if it was just \".\")\n",
    "        if not cleaned_text:\n",
    "            # print(f\"Filtering empty/punctuation mention: '{original_text}'\") # Optional debug\n",
    "            continue\n",
    "\n",
    "        # Filter 2: Filter if it's just a standalone title\n",
    "        if cleaned_text_lower in standalone_titles:\n",
    "            # print(f\"Filtering standalone title: '{original_text}' -> '{cleaned_text}'\") # Optional debug\n",
    "            continue\n",
    "\n",
    "\n",
    "        # Filter 3: Optional - Add more sophisticated checks if needed (e.g., for plurals)\n",
    "\n",
    "        # --- If it passes all filters, add it ---\n",
    "        # Decide whether to store the cleaned or original text for consolidation\n",
    "        mention_to_add = {\n",
    "            # Use cleaned_text if you want consolidation rules to work on \"Bennet\" instead of \". Bennet\"\n",
    "            \"text\": cleaned_text,\n",
    "            # Keep original if useful for reference\n",
    "            # \"original_text\": original_text,\n",
    "            \"start_char\": mention['start_char'],\n",
    "            \"end_char\": mention['end_char']\n",
    "        }\n",
    "        filtered_person_mentions.append(mention_to_add)\n",
    "\n",
    "    print(f\"DEBUG: Mentions remaining after filtering: {len(filtered_person_mentions)}\")\n",
    "else:\n",
    "     print(\"DEBUG: Initial person_mentions list was empty. Skipping filtering.\")\n",
    "\n",
    "\n",
    "# --- Save the FILTERED list ---\n",
    "# Make sure to save 'filtered_person_mentions' to your JSON file now\n",
    "output_filtered_path = \"../data/ner_person_mentions_bert_filtered.json\"\n",
    "if filtered_person_mentions:\n",
    "    print(f\"DEBUG: Saving {len(filtered_person_mentions)} filtered mentions to {output_filtered_path}...\")\n",
    "    import json\n",
    "    import os\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(output_filtered_path), exist_ok=True)\n",
    "        with open(output_filtered_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(filtered_person_mentions, f, indent=4)\n",
    "        print(f\"DEBUG: Successfully saved filtered mentions.\")\n",
    "    except Exception as e:\n",
    "        print(f\"DEBUG: Error saving filtered JSON: {e}\")\n",
    "else:\n",
    "     print(\"DEBUG: No filtered person mentions to save.\")\n",
    "\n",
    "\n",
    "# --- Proceed with Consolidation ---\n",
    "# Load the *filtered* JSON file in the next steps for consolidation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbac61ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Consolidate Characters using Alias Map - xml approach downn below\n",
    "## here is the one by avoiding using annotation and use nicknames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b524dadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3986 filtered mentions from ../data/ner_person_mentions_bert_filtered.json\n",
      "NickNamer initialized.\n",
      "Starting character consolidation using NickNamer and rules...\n",
      "Refining canonical keys based on frequency...\n",
      "\n",
      "Successfully saved consolidated results (using nicknames) to '../data/character_analysis_consolidated_nicknames.csv'\n"
     ]
    }
   ],
   "source": [
    "# --- Cell for Character Consolidation (in Notebook 01) ---\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "from nicknames import NickNamer # Import the library\n",
    "\n",
    "# --- Load Filtered Mentions ---\n",
    "filtered_mentions_path = \"../data/ner_person_mentions_bert_filtered.json\" # Path to filtered mentions\n",
    "try:\n",
    "    with open(filtered_mentions_path, 'r', encoding='utf-8') as f:\n",
    "        filtered_person_mentions = json.load(f)\n",
    "    print(f\"Loaded {len(filtered_person_mentions)} filtered mentions from {filtered_mentions_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading filtered mentions JSON: {e}\")\n",
    "    filtered_person_mentions = []\n",
    "\n",
    "# --- Initialize NickNamer ---\n",
    "try:\n",
    "    nn = NickNamer()\n",
    "    print(\"NickNamer initialized.\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not initialize NickNamer. Nickname lookup disabled. Error: {e}\")\n",
    "    nn = None # Disable nickname lookup if initialization fails\n",
    "\n",
    "# --- Define Titles (for fallback/normalization if not a nickname) ---\n",
    "# (You might already have this list defined earlier in the notebook)\n",
    "titles = {\"mr\", \"mrs\", \"miss\", \"ms\", \"dr\", \"lady\", \"sir\", \"colonel\", \"captain\", \"lord\"} # Lowercase\n",
    "\n",
    "# --- Consolidation Logic ---\n",
    "consolidated_characters = defaultdict(lambda: {\"count\": 0, \"variations\": set()})\n",
    "mention_texts_for_fallback = [] # Collect texts to determine fallback keys\n",
    "\n",
    "if filtered_person_mentions:\n",
    "    print(\"Starting character consolidation using NickNamer and rules...\")\n",
    "    for mention_data in filtered_person_mentions:\n",
    "        mention_text = mention_data['text'] # Assumes 'text' key holds the cleaned text from filtering step\n",
    "        mention_lower = mention_text.lower()\n",
    "\n",
    "        canonical_base = None\n",
    "\n",
    "        # --- Step 1: Check Nickname Dictionary ---\n",
    "        if nn: # Only if NickNamer initialized successfully\n",
    "            formal_names = nn.canonicals_of(mention_lower)\n",
    "            if formal_names:\n",
    "                # Found potential formal name(s)\n",
    "                # Simple strategy: use the first one found.\n",
    "                # Could be enhanced to handle multiple formal names if needed.\n",
    "                canonical_base = list(formal_names)[0]\n",
    "                # print(f\"DEBUG: Nickname mapping: '{mention_text}' -> '{canonical_base}'\") # Optional debug\n",
    "\n",
    "        # --- Step 2: Fallback to Rule-Based Normalization (if not found in nicknames) ---\n",
    "        if canonical_base is None:\n",
    "            parts = mention_text.split()\n",
    "            # Simple title stripping (if title is first word)\n",
    "            if parts and parts[0].lower().strip('.') in titles:\n",
    "                 canonical_base = \" \".join(parts[1:]) # Use name after title\n",
    "            else:\n",
    "                 canonical_base = mention_text # Use original (cleaned) text\n",
    "\n",
    "            # If after stripping title, the name is empty, maybe use original text?\n",
    "            if not canonical_base:\n",
    "                 canonical_base = mention_text\n",
    "\n",
    "            canonical_base = canonical_base.lower() # Ensure consistent casing for grouping\n",
    "\n",
    "        # --- Grouping ---\n",
    "        # Use the derived canonical_base for grouping\n",
    "        # Let's refine the canonical key later based on frequency\n",
    "        consolidated_characters[canonical_base][\"count\"] += 1\n",
    "        consolidated_characters[canonical_base][\"variations\"].add(mention_text) # Add original mention text as variation\n",
    "        mention_texts_for_fallback.append(mention_text)\n",
    "\n",
    "    # --- Step 3: Refine Canonical Keys (Example: Use most frequent variation as key) ---\n",
    "    print(\"Refining canonical keys based on frequency...\")\n",
    "    final_consolidated = defaultdict(lambda: {\"count\": 0, \"variations\": set()})\n",
    "    # Count frequencies of original mentions\n",
    "    mention_counts = Counter(mention_texts_for_fallback)\n",
    "\n",
    "    processed_bases = set()\n",
    "    for base, data in consolidated_characters.items():\n",
    "        if base in processed_bases: continue # Avoid reprocessing if already handled\n",
    "\n",
    "        # Find the most frequent original mention text associated with this base group\n",
    "        most_frequent_variation = base # Default to the base itself\n",
    "        max_freq = 0\n",
    "        current_group_variations = data[\"variations\"]\n",
    "\n",
    "        for variation in current_group_variations:\n",
    "             if mention_counts[variation] > max_freq:\n",
    "                 max_freq = mention_counts[variation]\n",
    "                 most_frequent_variation = variation\n",
    "\n",
    "        # Create the final canonical key (e.g., Title_Case, underscore for space)\n",
    "        # Handle potential titles still present if base == variation\n",
    "        parts = most_frequent_variation.split()\n",
    "        final_key_parts = []\n",
    "        title_prefix = \"\"\n",
    "        start_index = 0\n",
    "        if parts and parts[0].lower().strip('.') in titles:\n",
    "             title_prefix = parts[0].strip('.').capitalize() + \"_\"\n",
    "             start_index = 1\n",
    "\n",
    "        final_key_parts = [part.capitalize() for part in parts[start_index:]]\n",
    "        final_canonical_key = title_prefix + \"_\".join(final_key_parts)\n",
    "\n",
    "        # If key ends up empty, use a fallback\n",
    "        if not final_canonical_key:\n",
    "            final_canonical_key = f\"Unknown_{base[:10]}\" # Fallback key\n",
    "\n",
    "        # Merge data for this group under the final key\n",
    "        final_consolidated[final_canonical_key][\"count\"] += data[\"count\"]\n",
    "        final_consolidated[final_canonical_key][\"variations\"].update(data[\"variations\"])\n",
    "        processed_bases.add(base)\n",
    "\n",
    "\n",
    "    # --- Convert sets to lists for saving ---\n",
    "    final_output_list = []\n",
    "    for key, data in final_consolidated.items():\n",
    "        final_output_list.append({\n",
    "            \"canonical_key\": key,\n",
    "            \"total_mentions\": data[\"count\"],\n",
    "            \"variations\": sorted(list(data[\"variations\"])), # Sorted list\n",
    "            \"variation_count\": len(data[\"variations\"])\n",
    "            # Note: gender_from_xml is removed as we are not using XML aliases\n",
    "        })\n",
    "\n",
    "    # Sort by total mentions (descending)\n",
    "    final_output_list.sort(key=lambda x: x['total_mentions'], reverse=True)\n",
    "\n",
    "    # --- Save to CSV ---\n",
    "    output_csv = \"../data/character_analysis_consolidated_nicknames.csv\" # New name\n",
    "    output_json = \"../data/character_groups_consolidated_nicknames.json\" # New name\n",
    "    try:\n",
    "        df_consolidated = pd.DataFrame(final_output_list)\n",
    "        df_consolidated.to_csv(output_csv, index=False)\n",
    "        # Save the dict version to JSON as before (optional)\n",
    "        # with open(output_json, 'w', encoding='utf-8') as f:\n",
    "        #     json.dump(final_consolidated, f, indent=4, default=lambda x: list(x) if isinstance(x, set) else x) # Handle sets for JSON\n",
    "        print(f\"\\nSuccessfully saved consolidated results (using nicknames) to '{output_csv}'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError saving consolidated results: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nSkipping consolidation as no filtered PERSON mentions were loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_aliases_md",
   "metadata": {},
   "source": [
    "## Load Aliases from Annotated XML\n",
    "\n",
    "To improve character consolidation (e.g., grouping 'Lizzy' with 'Elizabeth'), we load the character list and aliases from the annotated XML file (`pp_full.xml`). This acts as a lookup table based on our 'training' data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "load_aliases_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading aliases from ../data/pp_full.xml...\n",
      "Created alias map with 100 entries.\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Load aliases from XML\n",
    "xml_file_path = \"../data/pp_full.xml\" # Path to the annotated file\n",
    "alias_to_canonical = {}\n",
    "canonical_name_gender = {} # Store gender info from XML as well\n",
    "\n",
    "print(f\"Loading aliases from {xml_file_path}...\")\n",
    "try:\n",
    "    tree = ET.parse(xml_file_path)\n",
    "    root = tree.getroot()\n",
    "    characters_element = root.find('characters')\n",
    "    \n",
    "    if characters_element is not None:\n",
    "        for character in characters_element.findall('character'):\n",
    "            canonical_name = character.get('name')\n",
    "            aliases_str = character.get('aliases')\n",
    "            gender = character.get('gender') # Get gender attribute\n",
    "            \n",
    "            if not canonical_name:\n",
    "                continue # Skip if character has no canonical name\n",
    "                \n",
    "            # Store gender for the canonical name\n",
    "            if gender:\n",
    "                canonical_name_gender[canonical_name] = gender.capitalize()\n",
    "            \n",
    "            # Add canonical name itself to map (normalized)\n",
    "            normalized_canonical = canonical_name.lower().strip()\n",
    "            if normalized_canonical:\n",
    "                 alias_to_canonical[normalized_canonical] = canonical_name\n",
    "            \n",
    "            # Add aliases to map (normalized)\n",
    "            if aliases_str:\n",
    "                aliases = aliases_str.split(';')\n",
    "                for alias in aliases:\n",
    "                    normalized_alias = alias.lower().strip()\n",
    "                    if normalized_alias:\n",
    "                        # Simple approach: map alias to canonical name\n",
    "                        # More complex: handle ambiguous aliases like 'Miss Bennet' if needed\n",
    "                        if normalized_alias not in alias_to_canonical: # Avoid overwriting if already mapped\n",
    "                             alias_to_canonical[normalized_alias] = canonical_name\n",
    "                        # else: print(f\"Ambiguous or duplicate alias ignored: {normalized_alias}\")\n",
    "                             \n",
    "        print(f\"Created alias map with {len(alias_to_canonical)} entries.\")\n",
    "        # print(\"Sample alias map:\", dict(list(alias_to_canonical.items())[:15]))\n",
    "        # print(\"Canonical genders:\", canonical_name_gender)\n",
    "    else:\n",
    "        print(\"Error: <characters> tag not found in XML.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: XML file not found at {xml_file_path}\")\n",
    "except ET.ParseError as pe:\n",
    "    print(f\"Error parsing XML file {xml_file_path}: {pe}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred loading aliases: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidate_chars_md",
   "metadata": {},
   "source": [
    "## Consolidate Characters using Alias Map\n",
    "\n",
    "Now, group the PERSON mentions found by NER using the alias map. If a mention matches an alias, group it under the canonical name. Otherwise, use fallback logic (e.g., normalized name or surname)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "consolidate_chars_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidating mentions using alias map...\n",
      "Consolidation complete. Processed 3986 mentions.\n",
      "Mapped 2969 mentions using aliases.\n",
      "Resulting unique character keys: 206\n",
      "\n",
      "Top characters after consolidation:\n",
      "           canonical_key  total_mentions  \\\n",
      "25      Elizabeth_Bennet             803   \n",
      "24              Mr_Darcy             406   \n",
      "19           Jane_Bennet             296   \n",
      "36            Mrs_Bennet             286   \n",
      "20            Mr_Bingley             235   \n",
      "17            Mr_Wickham             211   \n",
      "21               Collins             178   \n",
      "16          Lydia_Bennet             176   \n",
      "50        Lady_Catherine             132   \n",
      "62             Charlotte             113   \n",
      "81      Caroline_Bingley              98   \n",
      "139             Gardiner              80   \n",
      "54          Kitty_Bennet              72   \n",
      "76           Sir_William              42   \n",
      "7            Mary_Bennet              42   \n",
      "60            Miss Darcy              40   \n",
      "80                 Hurst              32   \n",
      "150  Colonel_Fitzwilliam              30   \n",
      "45               Collins              27   \n",
      "109             Gardiner              27   \n",
      "\n",
      "                                            variations  variation_count  \\\n",
      "25   Eliza, Eliza Bennet, Elizabeth, Elizabeth Benn...               10   \n",
      "24                     Darcy, Darcy, Fitzwilliam Darcy                3   \n",
      "19                                   Jane, Jane Bennet                2   \n",
      "36                                      Bennet, Bennet                2   \n",
      "20                                    Bingley, Bingley                2   \n",
      "17                    Wickham, George Wickham, Wickham                3   \n",
      "21                                             Collins                1   \n",
      "16   Lydia, Lydia Bennet, Miss Lydia, Miss Lydia Be...                4   \n",
      "50                           Catherine, Lady Catherine                2   \n",
      "62              Charlotte, Charlotte Lucas, Miss Lucas                3   \n",
      "81            Caroline, Caroline Bingley, Miss Bingley                3   \n",
      "139                                           Gardiner                1   \n",
      "54                                               Kitty                1   \n",
      "76                      Sir William, Sir William Lucas                2   \n",
      "7                                                 Mary                1   \n",
      "60                                          Miss Darcy                1   \n",
      "80                                               Hurst                1   \n",
      "150                                Colonel Fitzwilliam                1   \n",
      "45                                             Collins                1   \n",
      "109                                           Gardiner                1   \n",
      "\n",
      "    gender_from_xml  \n",
      "25           Female  \n",
      "24             Male  \n",
      "19           Female  \n",
      "36           Female  \n",
      "20             Male  \n",
      "17             Male  \n",
      "21             None  \n",
      "16           Female  \n",
      "50           Female  \n",
      "62           Female  \n",
      "81           Female  \n",
      "139            None  \n",
      "54           Female  \n",
      "76             Male  \n",
      "7            Female  \n",
      "60             None  \n",
      "80             None  \n",
      "150            Male  \n",
      "45             None  \n",
      "109            None  \n",
      "\n",
      "Consolidated character analysis data saved to '../data/character_analysis_consolidated.csv' and '../data/character_groups_consolidated.json'\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Consolidate character mentions using the alias map\n",
    "\n",
    "character_groups = {}\n",
    "mentions_processed = 0\n",
    "mentions_mapped = 0\n",
    "\n",
    "#print(person_mentions)\n",
    "\n",
    "if filtered_person_mentions: # Check if NER ran successfully\n",
    "    print(\"Consolidating mentions using alias map...\")\n",
    "    for mention in filtered_person_mentions:\n",
    "        mentions_processed += 1\n",
    "        original_mention_text = mention['text']\n",
    "        normalized_mention = original_mention_text.lower().strip()\n",
    "        \n",
    "        # Attempt to find canonical name using the alias map\n",
    "        canonical_name = alias_to_canonical.get(normalized_mention)\n",
    "        \n",
    "        char_key = None\n",
    "        if canonical_name:\n",
    "            # Found in alias map, use the canonical name as the key\n",
    "            char_key = canonical_name\n",
    "            mentions_mapped += 1\n",
    "        else:\n",
    "            # Fallback: Use the original mention text as key for now\n",
    "            # More sophisticated fallback (like surname grouping) could be added here\n",
    "            # We also might want to filter out non-names like 'Meryton' here\n",
    "            # Simple filter: skip if it looks like a place (e.g., capitalized, maybe check against known places)\n",
    "            # For now, let's just use the original text as key if no alias match\n",
    "            char_key = original_mention_text \n",
    "            # Basic check to avoid adding obvious non-names if desired\n",
    "            if char_key in ['Meryton', 'London', 'Hertfordshire', 'Kent', 'Derbyshire']: # Example filter\n",
    "                 continue\n",
    "            \n",
    "        # Add to groups\n",
    "        if char_key not in character_groups:\n",
    "            character_groups[char_key] = {\n",
    "                'variations': set(),\n",
    "                'count': 0\n",
    "            }\n",
    "        \n",
    "        character_groups[char_key]['variations'].add(original_mention_text)\n",
    "        character_groups[char_key]['count'] += 1\n",
    "        \n",
    "    print(f\"Consolidation complete. Processed {mentions_processed} mentions.\")\n",
    "    print(f\"Mapped {mentions_mapped} mentions using aliases.\")\n",
    "    print(f\"Resulting unique character keys: {len(character_groups)}\")\n",
    "\n",
    "    # --- Create DataFrame for analysis ---\n",
    "    character_data_list = []\n",
    "    for key, data in character_groups.items():\n",
    "         # Get gender from XML if available for this canonical key\n",
    "         xml_gender = canonical_name_gender.get(key, None) \n",
    "         character_data_list.append({\n",
    "            'canonical_key': key, # Changed 'key' to 'canonical_key'\n",
    "            'total_mentions': data['count'],\n",
    "            'variations': ', '.join(sorted(list(data['variations']))), # Sort variations for consistency\n",
    "            'variation_count': len(data['variations']),\n",
    "            'gender_from_xml': xml_gender # Add gender from XML if found\n",
    "        })\n",
    "        \n",
    "    character_df = pd.DataFrame(character_data_list)\n",
    "\n",
    "    # Sort by total mentions to see main characters\n",
    "    character_df = character_df.sort_values('total_mentions', ascending=False)\n",
    "\n",
    "    print(\"\\nTop characters after consolidation:\")\n",
    "    print(character_df.head(20))\n",
    "\n",
    "    # --- Save the structured character data ---\n",
    "    output_csv = '../data/character_analysis_consolidated.csv'\n",
    "    output_json = '../data/character_groups_consolidated.json'\n",
    "    \n",
    "    try:\n",
    "        character_df.to_csv(output_csv, index=False)\n",
    "        # Save JSON compatible format\n",
    "        save_groups = {k: {'variations': list(v['variations']), 'count': v['count']} \n",
    "                       for k, v in character_groups.items()}\n",
    "        with open(output_json, 'w', encoding='utf-8') as f:\n",
    "            json.dump(save_groups, f, indent=4, ensure_ascii=False)\n",
    "        print(f\"\\nConsolidated character analysis data saved to '{output_csv}' and '{output_json}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError saving consolidated results: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"\\nSkipping consolidation as no PERSON mentions were extracted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next_steps_consolidation_md",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1.  **Review Output:** Check the new `character_analysis_consolidated.csv`. Does it correctly group 'Lizzy' under 'Elizabeth_Bennet'? Are other characters consolidated better?\n",
    "2.  **Refine Alias Map/Fallback:** You might need to refine how ambiguous aliases (like 'Miss Bennet') are handled or improve the fallback logic for mentions not in the alias map.\n",
    "3.  **Proceed to Gender Classification:** Now you can run the `02_gender_classification.ipynb` notebook, making sure it loads this *new* consolidated CSV (`character_analysis_consolidated.csv`). The gender classification should now operate on the canonical keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa9f7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3986 filtered mentions from ../data/ner_person_mentions_bert_filtered.json\n",
      "Found 235 unique names for fuzzy matching.\n",
      "Starting fuzzy matching with threshold 91...\n",
      "Fuzzy matching finished. Found 205 groups.\n",
      "\n",
      "--- Fuzzy Matched Groups ---\n",
      "Group ' Addison': [' Addison', 'Addison']\n",
      "Group ' Austen Leigh': [' Austen Leigh', 'Austen Leigh']\n",
      "Group ' Bennet': [' Bennet', 'Bennet', 'Bennets']\n",
      "Group ' Bingley': [' Bingley', 'Bingley', 'Bingleys']\n",
      "Group ' Collins': [' Collins', 'Collins']\n",
      "Group ' Darcy': [' Darcy', 'Darcy']\n",
      "Group ' Denny': [' Denny', 'Denny']\n",
      "Group ' Forster': [' Forster', 'Forster', 'Forsters']\n",
      "Group ' Gardiner': [' Gardiner', 'Gardiner', 'Gardiners']\n",
      "Group ' Hill': [' Hill', 'Hill']\n",
      "Group ' Hurst': [' Hurst', 'Hurst']\n",
      "Group ' Jones': [' Jones', 'Jones']\n",
      "Group ' Long': [' Long', 'Long']\n",
      "Group ' Nichols': [' Nichols', 'Nicholls']\n",
      "Group ' Norris': [' Norris', 'Norris']\n",
      "Group ' Philips': [' Philips', 'Philips']\n",
      "Group ' Reynolds': [' Reynolds', 'Reynolds']\n",
      "Group ' Robinson': [' Robinson', 'Robinson']\n",
      "Group ' Wickham': [' Wickham', 'Wickham']\n",
      "Group ' Younge': [' Younge', 'Younge']\n",
      "Group 'Catherin': ['Catherin', 'Catherine']\n",
      "Group 'De': ['De', 'de']\n",
      "Group 'Elizab': ['Elizab', 'Elizabe']\n",
      "Group 'Goulding': ['Goulding', 'Gouldings']\n",
      "Group 'Miss Bennet': ['Miss Bennet', 'Miss Bennets']\n",
      "Group 'Miss De Bourgh': ['Miss De Bourgh', 'Miss de Bourgh']\n",
      "\n",
      "Displayed 26 groups with multiple variations.\n",
      "\n",
      "Successfully saved fuzzy matched groups to ../data/character_groups_fuzzy_matched.json\n"
     ]
    }
   ],
   "source": [
    "# Cell: Fuzzy Matching for Character Consolidation (Corrected)\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from thefuzz import fuzz # Or from fuzzywuzzy import fuzz\n",
    "from thefuzz import process # Or from fuzzywuzzy import process\n",
    "\n",
    "# --- Configuration ---\n",
    "filtered_mentions_path = \"../data/ner_person_mentions_bert_filtered.json\" # Path to filtered mentions\n",
    "similarity_threshold = 91 # Adjust this threshold (0-100) as needed\n",
    "output_grouped_path = \"../data/character_groups_fuzzy_matched.json\" # Optional: Save results\n",
    "\n",
    "# --- Load Filtered Mentions ---\n",
    "try:\n",
    "    with open(filtered_mentions_path, 'r', encoding='utf-8') as f:\n",
    "        filtered_person_mentions = json.load(f)\n",
    "    print(f\"Loaded {len(filtered_person_mentions)} filtered mentions from {filtered_mentions_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading filtered mentions JSON: {e}\")\n",
    "    filtered_person_mentions = []\n",
    "\n",
    "# --- Extract Unique Names ---\n",
    "unique_names = sorted(list(set(mention['text'] for mention in filtered_person_mentions)))\n",
    "print(f\"Found {len(unique_names)} unique names for fuzzy matching.\")\n",
    "\n",
    "# --- Perform Fuzzy Matching ---\n",
    "grouped_names = defaultdict(list)\n",
    "processed_names = set() # Keep track of names already assigned to a group\n",
    "\n",
    "if unique_names:\n",
    "    print(f\"Starting fuzzy matching with threshold {similarity_threshold}...\")\n",
    "    for name in unique_names:\n",
    "        if name in processed_names:\n",
    "            continue # Skip if already part of a group\n",
    "\n",
    "        # Create a list of names not yet processed\n",
    "        choices = [n for n in unique_names if n not in processed_names]\n",
    "\n",
    "        # Find similar names\n",
    "        similar_matches = process.extractBests(name, choices, scorer=fuzz.token_sort_ratio, score_cutoff=similarity_threshold, limit=None) # limit=None gets all matches above cutoff\n",
    "\n",
    "        # The base name for the group will be the current 'name'\n",
    "        current_group = [name]\n",
    "        processed_names.add(name)\n",
    "\n",
    "        # Add similar matches found to the group and mark them as processed\n",
    "        # --- CORRECTED LINE BELOW ---\n",
    "        for match, score in similar_matches:\n",
    "            if match != name: # Don't add the name itself again\n",
    "                 current_group.append(match)\n",
    "                 processed_names.add(match)\n",
    "                 # print(f\"  Grouping '{match}' with '{name}' (Score: {score})\") # Optional debug\n",
    "\n",
    "        # Use the first name in the sorted group as the canonical key (or keep 'name')\n",
    "        canonical_key = sorted(current_group)[0]\n",
    "        grouped_names[canonical_key].extend(current_group)\n",
    "\n",
    "    print(f\"Fuzzy matching finished. Found {len(grouped_names)} groups.\")\n",
    "\n",
    "    # --- Print Groups (Optional) ---\n",
    "    print(\"\\n--- Fuzzy Matched Groups ---\")\n",
    "    group_count = 0\n",
    "    for key, names in grouped_names.items():\n",
    "        if len(names) > 1: # Only print groups with more than one variation\n",
    "             print(f\"Group '{key}': {sorted(list(set(names)))}\") # Show unique names per group\n",
    "             group_count += 1\n",
    "    print(f\"\\nDisplayed {group_count} groups with multiple variations.\")\n",
    "\n",
    "    # --- Optional: Save Grouped Results ---\n",
    "    try:\n",
    "        # Convert sets to lists for JSON compatibility if needed, though list is used above\n",
    "        save_data = {key: sorted(list(set(names))) for key, names in grouped_names.items()}\n",
    "        with open(output_grouped_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(save_data, f, indent=2)\n",
    "        print(f\"\\nSuccessfully saved fuzzy matched groups to {output_grouped_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError saving fuzzy matched groups: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping fuzzy matching as no unique names were found.\")\n",
    "\n",
    "# --- NEXT STEPS ---\n",
    "# Remember to adapt the next cell (b524dadd) to use 'grouped_names' or the output file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
